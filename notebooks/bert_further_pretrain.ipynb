{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cFLM26T_aJ3J",
        "YlzWfMqn2F6X",
        "VxgniVi3-_2S",
        "82RbERuGTTmi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "37192a5e9a9e44adae45a1a26c1b8e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_961f00545e8e46e0840cd026acecd321",
              "IPY_MODEL_a6938c030491414984ad6a362cdff13f",
              "IPY_MODEL_a929e75db198490bad00a220dd660400"
            ],
            "layout": "IPY_MODEL_972912329b1246b983f7365d1fca4d59"
          }
        },
        "961f00545e8e46e0840cd026acecd321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d6896a963d74b6ba3995251f6330a5e",
            "placeholder": "​",
            "style": "IPY_MODEL_a5306b39c05f4d748fef0c1911ef058d",
            "value": "100%"
          }
        },
        "a6938c030491414984ad6a362cdff13f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12d2745178894c79b639d8ba56ae7a70",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12d7e4653bbf4f05a40a171ee9afc907",
            "value": 1
          }
        },
        "a929e75db198490bad00a220dd660400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_708cdad6e70842d8a71fba3d94333b52",
            "placeholder": "​",
            "style": "IPY_MODEL_df5929c9da8a40e0b58da5e1a1e95fbe",
            "value": " 1/1 [00:00&lt;00:00, 27.69it/s]"
          }
        },
        "972912329b1246b983f7365d1fca4d59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d6896a963d74b6ba3995251f6330a5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5306b39c05f4d748fef0c1911ef058d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12d2745178894c79b639d8ba56ae7a70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12d7e4653bbf4f05a40a171ee9afc907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "708cdad6e70842d8a71fba3d94333b52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df5929c9da8a40e0b58da5e1a1e95fbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "zHHDHAkK6s70"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "xbzlwFj9PRO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installations"
      ],
      "metadata": {
        "id": "yd_ioI_qPhsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install transformers seqeval[gpu]"
      ],
      "metadata": {
        "id": "jNPr95m6PT5i"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install fugashi\n",
        "!pip install ipadic"
      ],
      "metadata": {
        "id": "iGX1m692PbVL"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "zKHt8FTzXLVr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4076a5d7-f5b2-4a58-e399-3128efe340d9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwu25DJS0kcB",
        "outputId": "8d9b5662-f4ba-4385-e58b-4e6f99b7d388"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.11.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "bmR-wqZ5PjfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer \n",
        "from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification \n",
        "\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "zaqEX756PlGK"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M1Gkx0vPn2r",
        "outputId": "67bbb236-e9c0-4f71-c6a1-5e058ec5b697"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42"
      ],
      "metadata": {
        "id": "9zJqNkaZ10CI"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Japanese BERT"
      ],
      "metadata": {
        "id": "YkZig3aiVntp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"cl-tohoku/bert-base-japanese\""
      ],
      "metadata": {
        "id": "dNdxMXLrVmxq"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiMHvK5uejKb",
        "outputId": "ccfc5512-399d-43dc-aa98-b216b888db46"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese/snapshots/5dc6dbba88a42d21da3b71025c109c42462307f2/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese/snapshots/5dc6dbba88a42d21da3b71025c109c42462307f2/vocab.txt\n",
            "loading file spiece.model from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese/snapshots/5dc6dbba88a42d21da3b71025c109c42462307f2/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese/snapshots/5dc6dbba88a42d21da3b71025c109c42462307f2/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForMaskedLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT6VJLbVxWTp",
        "outputId": "041d8bcb-b719-48de-9350-808998dee36a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese/snapshots/5dc6dbba88a42d21da3b71025c109c42462307f2/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese/snapshots/5dc6dbba88a42d21da3b71025c109c42462307f2/pytorch_model.bin\n",
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertForMaskedLM were initialized from the model checkpoint at cl-tohoku/bert-base-japanese.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LCQCSTdHIexp"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example of a title: (Tokinization)"
      ],
      "metadata": {
        "id": "cFLM26T_aJ3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding example"
      ],
      "metadata": {
        "id": "NjT1MZzet0Dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See tutorial : [Hugging Face](https://huggingface.co/course/chapter2/5?fw=pt)"
      ],
      "metadata": {
        "id": "_m830coiuW9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"「東海道五十三次」  「三十八」「藤川」\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUujAND2YTSk",
        "outputId": "44421dd2-abb2-4fac-edf7-3d9ceaa6f133"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['「', '東海道', '五', '十', '三', '次', '」', '「', '三', '十', '八', '」', '「', '藤', '##川', '」']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJZ_9T90Ybbz",
        "outputId": "fb88ba96-b089-451b-91bc-361a35633fe6"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[36, 7174, 989, 714, 240, 288, 38, 36, 240, 714, 1035, 38, 36, 1408, 28698, 38]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_string = tokenizer.decode(ids)\n",
        "print(decoded_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e_wzFJHq5BC",
        "outputId": "4d0a4f05-d151-47e6-e3d7-15aa11099217"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "「 東海道 五 十 三 次 」 「 三 十 八 」 「 藤川 」\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Batch input"
      ],
      "metadata": {
        "id": "q6RqlpCHetPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The follow code fail (see comment cell), because we sent a single sequence to the model, whereas Hugging Face Transformers models expect multiple sentences by default. \n",
        "`ValueError: not enough values to unpack (expected 2, got 1)`. \n",
        "\n",
        "So, we need to add into batch (eg 2 titles -> batched_tokens = [\n",
        "    [\"「\", \"東海道\", \"五\"],\n",
        "    [\"「\", \"藤\"]\n",
        "] -> batched_ids = [\n",
        "    [36, 7174, 989],\n",
        "    [36, 45]\n",
        "])"
      ],
      "metadata": {
        "id": "Tm6QuywUZjrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_temp = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "5TSROfYQu_Qp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691d59e4-018c-4714-ca62-a6a36c0a9592"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese/snapshots/5dc6dbba88a42d21da3b71025c109c42462307f2/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese/snapshots/5dc6dbba88a42d21da3b71025c109c42462307f2/pytorch_model.bin\n",
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input_ids = torch.tensor(ids)\n",
        "\n",
        "# # This line will fail.\n",
        "# model(input_ids)"
      ],
      "metadata": {
        "id": "KkXuXge6ZKqD"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([ids]) ### include in batch (eg batch_id = [ids1,ids2]) : Very important !\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model_temp(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a6qNaa7ZaPs",
        "outputId": "2a692a55-d365-46e2-c49d-2cb06fb662cf"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[   36,  7174,   989,   714,   240,   288,    38,    36,   240,   714,\n",
            "          1035,    38,    36,  1408, 28698,    38]])\n",
            "Logits: tensor([[0.6940, 0.1056]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Padding token\n",
        "\n",
        "The padding token ID can be found in tokenizer.`pad_token_id`. "
      ],
      "metadata": {
        "id": "Aub9OM46czUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Padding id of Japanese Bert: ',tokenizer.pad_token_id)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChpZyk3ZZaMj",
        "outputId": "2e73e798-44af-4cc4-809e-092b37dc0f9b"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padding id of Japanese Bert:  0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There’s something wrong with the logits in our batched predictions: the second row should be the same as the logits for the second sentence, but we’ve got completely different values!\n",
        "\n",
        "This is because the key feature of Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask."
      ],
      "metadata": {
        "id": "3XI-AhAbeUaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model_temp(torch.tensor(sequence1_ids)).logits)\n",
        "print(model_temp(torch.tensor(sequence2_ids)).logits)\n",
        "print(\"VS\")\n",
        "print(model_temp(torch.tensor(batched_ids)).logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0eeZcvKZaGK",
        "outputId": "72cc8426-0227-42a4-c108-5c6a48db3f2f"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2836, -0.1544]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 0.2651, -0.2056]], grad_fn=<AddmmBackward0>)\n",
            "VS\n",
            "tensor([[ 0.2836, -0.1544],\n",
            "        [ 0.4044, -0.2048]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Attention mask"
      ],
      "metadata": {
        "id": "YzLdG0tWe1Yq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model)."
      ],
      "metadata": {
        "id": "3te-LxZwfD_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model_temp(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "\n",
        "print(model_temp(torch.tensor(sequence1_ids)).logits)\n",
        "print(model_temp(torch.tensor(sequence2_ids)).logits)\n",
        "print(\"VS\")\n",
        "print(outputs.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IPV1VoEZaDV",
        "outputId": "0ef27dc2-59dd-4952-a0b9-73ebd6f93507"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2836, -0.1544]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 0.2651, -0.2056]], grad_fn=<AddmmBackward0>)\n",
            "VS\n",
            "tensor([[ 0.2836, -0.1544],\n",
            "        [ 0.2651, -0.2055]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Sorter sequences: padding\n",
        "\n",
        "vs\n",
        "\n",
        "Longer sequences: truncation\n",
        "\n",
        "With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:\n",
        "\n",
        "- Use a model with a longer supported sequence length.\n",
        "- Truncate your sequences.\n",
        "\n",
        "Otherwise, we recommend you truncate your sequences by specifying the max_sequence_length parameter:\n",
        "\n",
        "\n",
        "`sequence = sequence[:max_sequence_length]`"
      ],
      "metadata": {
        "id": "aQw5SQdzqEsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run example"
      ],
      "metadata": {
        "id": "Kv6IfhzHuLV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\n",
        "   \"「東海道五十三次」  「三十八」「藤川」\",\n",
        "   \"「東都六玉顔ノ内」  「角田川」\",\n",
        "] # 1) batch of sentencies"
      ],
      "metadata": {
        "id": "dUW74ofZZZ_p"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer(sequences, \n",
        "                  padding=True, # 2) & 3) Sorter sequences: add padding and attention_mask token id until fill the maximum length accepted by the model\n",
        "                  truncation=True, # 4) Longer sequences: truncate a sequence to the maximum length accepted by the model\n",
        "                  return_tensors=\"pt\"\n",
        "                  )\n",
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXFYTTYNZZ8y",
        "outputId": "89e9ed22-b82f-44fa-abc5-3683b78e1f0d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[    2,    36,  7174,   989,   714,   240,   288,    38,    36,   240,\n",
              "           714,  1035,    38,    36,  1408, 28698,    38,     3],\n",
              "        [    2,    36, 26503,  1688,  2631,  2679,   534,   186,    38,    36,\n",
              "         24334,   529,    38,     3,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "bsSktQVXX8jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n"
      ],
      "metadata": {
        "id": "O9ZE13wgYF7i"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # This is new\n",
        "# batch[\"labels\"] = torch.tensor([1, 1])\n",
        "\n",
        "# optimizer = AdamW(model_temp.parameters())\n",
        "# loss = model(**batch).loss\n",
        "# loss.backward()\n",
        "# optimizer.step()"
      ],
      "metadata": {
        "id": "OZ51tNM-TUCC"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning"
      ],
      "metadata": {
        "id": "onT0L5D2vRSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsupervise learning**\n",
        "\n",
        "Fine-tuning a masked language model:\n",
        "\n",
        "We want to first fine-tune the language models on meisho data, before training our task-specific head (NER). \n",
        "\n",
        "See tutorial: [Hugging Face](https://huggingface.co/course/chapter7/3?fw=pt)"
      ],
      "metadata": {
        "id": "jXoknE3uv9_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further train on Japanese BERT: Fine-tune a pretrained model with meisho dataset with over 20.000 titles"
      ],
      "metadata": {
        "id": "b5KYqdKrBX68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_num_parameters = model.num_parameters() / 1_000_000\n",
        "print(f\"'>>> Japanize BERT number of parameters: {round(bert_num_parameters)}M'\")\n",
        "print(f\"'>>> BERT number of parameters: 110M'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDKDzyrmvQEY",
        "outputId": "4774dc16-0cd5-4703-d65b-bf4bcfa9a642"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'>>> Japanize BERT number of parameters: 111M'\n",
            "'>>> BERT number of parameters: 110M'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset"
      ],
      "metadata": {
        "id": "Tl2hsJpixhs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- get meisho-e dataset (over 20.000 unlabeled title)\n",
        "- load in the format that accept hugging face, using `load_dataset` (see tutorial: [Hugginface](https://huggingface.co/course/chapter5/2?fw=pt))"
      ],
      "metadata": {
        "id": "-onNUPxcxmbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "PUzLjFu-xlMH"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"\"\n",
        "data_files = {\n",
        "    #\"unsupervised\": url + \"train_place.csv\",\n",
        "    \"unsupervised\": url + \"arc_meisho_full.csv\",\n",
        "    #\"unsupervised\": url + \"arc_meisho.csv\",\n",
        "}\n",
        "\n",
        "meisho_dataset = load_dataset(\"csv\", data_files=data_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "37192a5e9a9e44adae45a1a26c1b8e20",
            "961f00545e8e46e0840cd026acecd321",
            "a6938c030491414984ad6a362cdff13f",
            "a929e75db198490bad00a220dd660400",
            "972912329b1246b983f7365d1fca4d59",
            "3d6896a963d74b6ba3995251f6330a5e",
            "a5306b39c05f4d748fef0c1911ef058d",
            "12d2745178894c79b639d8ba56ae7a70",
            "12d7e4653bbf4f05a40a171ee9afc907",
            "708cdad6e70842d8a71fba3d94333b52",
            "df5929c9da8a40e0b58da5e1a1e95fbe"
          ]
        },
        "id": "UWXkOCGnxlk4",
        "outputId": "24bdc20c-fde3-4948-e7f9-3076b85c4399"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-f412c487edfea47d\n",
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-f412c487edfea47d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37192a5e9a9e44adae45a1a26c1b8e20"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meisho_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mCZDyzczEfQ",
        "outputId": "dc0a8b86-8402-429e-855c-a7366168f82a"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    unsupervised: Dataset({\n",
              "        features: ['title', 'link', 'full_title'],\n",
              "        num_rows: 20346\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# meisho_dataset[\"unsupervised\"]['title']"
      ],
      "metadata": {
        "id": "hT5GzMKmY0K_"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features/columns names of the dataset\n",
        "meisho_features = meisho_dataset[\"unsupervised\"].features.keys()\n",
        "\n",
        "meisho_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_LzqEYTSioE",
        "outputId": "61613f07-967b-4c98-8403-64713c7ce23a"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['title', 'link', 'full_title'])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example"
      ],
      "metadata": {
        "id": "YlzWfMqn2F6X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dnp8SMtJQhLK"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_text = \"「東海道五十三次」  「三十八」「藤川」\"\n",
        "text = \"「東海道五十三[MASK]」  「三十八」「藤川」\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "token_logits = model(**inputs).logits # predict probabilities\n",
        "\n",
        "# Find the location of [MASK] and extract its logits\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "\n",
        "# Pick the [MASK] candidates with the highest logits\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "for token in top_5_tokens:\n",
        "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPd-vv3Ou93v",
        "outputId": "b3343bfe-7c05-40f3-d4fe-2ea9c8021bbc"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'>>> 「東海道五十三次」  「三十八」「藤川」'\n",
            "'>>> 「東海道五十三駅」  「三十八」「藤川」'\n",
            "'>>> 「東海道五十三度」  「三十八」「藤川」'\n",
            "'>>> 「東海道五十三号線」  「三十八」「藤川」'\n",
            "'>>> 「東海道五十三[UNK]」  「三十八」「藤川」'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "aFPfY5KxBx0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# url = \"\"\n",
        "# data_files = {\n",
        "#     \"train\": url + \"test_place.csv\",\n",
        "#     \"test\": url + \"train_place.csv\",\n",
        "#     \"unsupervised\": url + \"arc_meisho.csv\",\n",
        "# }\n",
        "\n",
        "# meisho_dataset = load_dataset(\"csv\", data_files=data_files)\n",
        "\n",
        "# meisho_dataset"
      ],
      "metadata": {
        "id": "4-WW_AIL4GSK"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meisho_dataset[\"unsupervised\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gmBuBAWzJwX",
        "outputId": "9f14a05d-05b3-4163-bd91-343f987ca963"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': '芳年「東海道\\u3000京都之内」「大内能上覧図」',\n",
              " 'link': 'https://www.arc.ritsumei.ac.jp/archive01/theater/th_image/PB/arc/Prints/arcUP/arcUP0542.jpg',\n",
              " 'full_title': 'arcUP0542文久０３・・芳年「東海道\\u3000京都之内」「大内能上覧図」'}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = meisho_dataset[\"unsupervised\"].shuffle(seed=SEED).select(range(3))\n",
        "\n",
        "for row in sample:\n",
        "    print(f\"\\n'>>> Title: {row['title']}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhNdFxTM1tC6",
        "outputId": "b56e167b-fd90-4811-b635-25d7baf78e81"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-f412c487edfea47d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a34341f977258804.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "'>>> Title: 北斎「奧津」「江尻へ一リ卅丁」'\n",
            "\n",
            "'>>> Title: 歌麿「江戸名所十景」「王子社の雪」'\n",
            "\n",
            "'>>> Title: 「四月中松ノ尾祭」'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "kaoiJgLHwLAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Masked language modeling: \n",
        "\n",
        "A common preprocessing step is to concatenate all the examples and then split the whole corpus into chunks of equal size. This is quite different from our usual approach, where we simply tokenize individual examples. \n",
        "\n",
        "We concatenate everything together, because individual examples might get truncated if they’re too long, and that would result in losing information that might be useful for the language modeling task!\n",
        "\n",
        "(Important) We’ll first tokenize our corpus as usual, but **without** setting the `truncation=True` option in our tokenizer!!!"
      ],
      "metadata": {
        "id": "BZm7KuodwT5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####  1) Tokinize text"
      ],
      "metadata": {
        "id": "euV_maZJ-1Kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.is_fast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzZUL7G5FCzP",
        "outputId": "b3c71d7b-68ba-4e57-c3bf-10a9b8502dd7"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples, sentence_colname:str = \"title\"):\n",
        "    result = tokenizer(examples[sentence_colname])\n",
        "    if tokenizer.is_fast:\n",
        "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
        "    return result"
      ],
      "metadata": {
        "id": "zi0WUhTHu9yO"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = meisho_dataset.map(\n",
        "    tokenize_function, \n",
        "    batched=True,  # use batched=True to activate fast multithreading!\n",
        "    remove_columns=meisho_features  # remove all the initial column as we need only tokizised sentece (eg [\"title\", \"entities\"])\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_DKtFZbu9ut",
        "outputId": "33d7e75c-6e6c-421d-9b60-f27f421f11fc"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-f412c487edfea47d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-49816edbc2798eca.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbL0CFwZu9ru",
        "outputId": "2619f8d5-667c-4912-b89e-1985bd583734"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    unsupervised: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 20346\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2) Split into Chunk\n"
      ],
      "metadata": {
        "id": "VB2iEB4p72GZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Max length value is derived from the `tokenizer_config.json` file associated with a checkpoint; in this case, the context size is 512 tokens, just like with original BERT)"
      ],
      "metadata": {
        "id": "1CZegOKt6Wtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Japanese BERT max length of sentence:\", tokenizer.model_max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIzNg1Te3Mzi",
        "outputId": "ecbc4129-7019-4f66-fb9c-015a9364a722"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Japanese BERT max length of sentence: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to run our experiments on GPUs like those found on Google Colab, we’ll pick something a bit smaller that can fit in memory:"
      ],
      "metadata": {
        "id": "ijnKo5Zx6uCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 128"
      ],
      "metadata": {
        "id": "j04JRJBh3NCQ"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Example"
      ],
      "metadata": {
        "id": "VxgniVi3-_2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Concatenation of title into one"
      ],
      "metadata": {
        "id": "Dd1N60Yy-sH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Slicing produces a list of lists for each feature\n",
        "tokenized_samples = tokenized_datasets[\"unsupervised\"][:5]\n",
        "\n",
        "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
        "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV1hs_np6xUK",
        "outputId": "843a53da-a6ac-4818-89bf-0b01a42b8ef2"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'>>> Review 0 length: 17'\n",
            "'>>> Review 1 length: 15'\n",
            "'>>> Review 2 length: 18'\n",
            "'>>> Review 3 length: 13'\n",
            "'>>> Review 4 length: 15'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_examples = {\n",
        "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
        "}\n",
        "total_length = len(concatenated_examples[\"input_ids\"])\n",
        "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y_oPxe-8AAc",
        "outputId": "e790d658-fd80-41ee-b81f-6d628d66771d"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'>>> Concatenated reviews length: 78'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the concatenated titles into chunks of the size given by `block_size`/`chunk_size`."
      ],
      "metadata": {
        "id": "QUO4yXPR8MPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = {\n",
        "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "    for k, t in concatenated_examples.items()\n",
        "}\n",
        "\n",
        "for chunk in chunks[\"input_ids\"]:\n",
        "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTJ9mXgW8AXB",
        "outputId": "f5c16c55-d45d-4e5d-89dd-eb8a9377cf78"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'>>> Chunk length: 78'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last chunk will generally be smaller than the maximum chunk size. There are two main strategies for dealing with this:\n",
        "\n",
        "- Drop the last chunk if it’s smaller than chunk_size. (Implement above)\n",
        "- Pad the last chunk until its length equals chunk_size."
      ],
      "metadata": {
        "id": "qJvdyWb281Wx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Create chunck"
      ],
      "metadata": {
        "id": "0h01_-vW_MqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(examples, chunk_size:int = 128):\n",
        "\n",
        "    # Concatenate all texts\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "\n",
        "    # Compute length of concatenated texts\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]]) \n",
        "\n",
        "    # We drop the last chunk if it's smaller than chunk_size\n",
        "    total_length = (total_length // chunk_size) * chunk_size\n",
        "\n",
        "    # Split by chunks of max_len\n",
        "    result = {\n",
        "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "\n",
        "    # Create a new labels column\n",
        "    \"\"\" Create a new labels column\n",
        "    masked language modeling the objective is to predict randomly masked tokens\n",
        "    in the input batch, and by creating a labels column we provide the ground truth\n",
        "    for our language model to learn from.\n",
        "    \"\"\"\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "D6kis62H8AcJ"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
        "lm_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWAqqgQL8AeR",
        "outputId": "4d68fe70-f6f7-44aa-d890-95ae35303763"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-f412c487edfea47d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a92491581287bf5f.arrow\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    unsupervised: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 2995\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(lm_datasets[\"unsupervised\"][1][\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "KXKR_TFu8Ah6",
        "outputId": "613fb792-cc78-4fea-a2a2-5a6ecc7a1467"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'」 「 御供所 」 「 若宮 」 「 別 雷 皇 太 神宮 」 「 杉尾 社 」 「 仮 殿 」 [SEP] [CLS] 国貞 「 東海道 名所 之 内 」 「 京 加茂 」 「 山科 」 「 黒谷 」 「 吉田山 」 「 将軍 塚 」 「 比叡山 」 「 比良 」 [SEP] [CLS] 暁斎 「 東海道 名所 之 内 」 「 加茂 の 競馬 」 [SEP] [CLS] 豊国 「 東海道 名所 之 内 」 「 [UNK] 河原 」 「 [UNK] 川原 」 「 みたらし 川 」 「 河合 社 」 [SEP] [CLS] 芳 艶 「 東海道 名所 之 内 」 「 祇園 祭礼 」 [SEP] [CLS] 芳 幾 「 東海道 京都 名所 之 内'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(lm_datasets[\"unsupervised\"][1][\"labels\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "bY6IiE6G8AlT",
        "outputId": "44b8fa37-6630-4bac-b36d-73d1b6caf411"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'」 「 御供所 」 「 若宮 」 「 別 雷 皇 太 神宮 」 「 杉尾 社 」 「 仮 殿 」 [SEP] [CLS] 国貞 「 東海道 名所 之 内 」 「 京 加茂 」 「 山科 」 「 黒谷 」 「 吉田山 」 「 将軍 塚 」 「 比叡山 」 「 比良 」 [SEP] [CLS] 暁斎 「 東海道 名所 之 内 」 「 加茂 の 競馬 」 [SEP] [CLS] 豊国 「 東海道 名所 之 内 」 「 [UNK] 河原 」 「 [UNK] 川原 」 「 みたらし 川 」 「 河合 社 」 [SEP] [CLS] 芳 艶 「 東海道 名所 之 内 」 「 祇園 祭礼 」 [SEP] [CLS] 芳 幾 「 東海道 京都 名所 之 内'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3) Add [MASK] on labels"
      ],
      "metadata": {
        "id": "geRCGdglBvWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inserting [MASK] tokens at random positions in the inputs using `DataCollatorForLanguageModeling`\n",
        "\n",
        "In `DataCollatorForLanguageModeling`, the `mlm_probability` argument that specifies what fraction of the tokens to mask. We’ll pick 15%, which is the amount used for BERT and a common choice in the literature:"
      ],
      "metadata": {
        "id": "pFaowP7tBsqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
        "                                                mlm_probability=0.15)"
      ],
      "metadata": {
        "id": "KmaBFN4g8ApT"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "Q9bV21FxCm4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [lm_datasets[\"unsupervised\"][i] for i in range(2)]\n",
        "\n",
        "for chunk in data_collator(samples)[\"input_ids\"]:\n",
        "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdeM4iD58Ass",
        "outputId": "691157e5-cfb1-48c8-b143-b19207a38738"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "'>>> [CLS] 芳年 「 東海道 京都 之 [MASK] 」 「 大内 能 上覧 図 」 [SEP] [CLS] 豊国 「 東海道 京都 名所 之 内 [MASK] [MASK] 四条 河原 」 [SEP] [CLS] [MASK]斎 「 東海道 名所 之 内 」 「 御 能 拝見 之 図 」 [SEP] [CLS] 芳盛 「 東海道 」 「 京都 [MASK] 震 殿 」 [SEP] [CLS] 芳盛 「 東海 道之 内 」 「 京都 参内 」 [SEP] [CLS] 芳盛 「 東海 道 [MASK] 内 」 「 京 」 「 大内 [UNK] 之 遊覧 」 [SEP] [CLS] 豊国 「 東海道 名所 之 内 」 「 上加茂 」 [MASK] 岩 [MASK] 」 「 三本杉 」 「 片岡 [MASK] 」 「 楼門'\n",
            "\n",
            "'>>> 」 「 [MASK]供 [MASK] [MASK] 「 若宮 [MASK] 「 [MASK] 雷 皇 太 神宮 」 「 杉尾 社 」 「 仮 殿 」 [SEP] [CLS] 国 [MASK] 「 [MASK] 名所 之 内 」 「 京 加茂 」 「 山科 」 「 黒谷 」 「 [MASK]山 」 「 将軍 塚 」 [MASK] 比叡山 」 「 比良 」 [SEP] [CLS] 暁斎 「 東海道 名所 之 [MASK] [MASK] [MASK] 加茂 [MASK] 競馬 」 [SEP] [CLS] 豊国 「 東海道 名所 之 内 戦時 [MASK] [UNK] [MASK] 」 「 [UNK] 川原 」 「 みたらし 川 」 「 河合 社 」 [SEP] [CLS] 芳 艶 「 東海道 名所 之 内 」 「 祇園 祭礼 」 [SEP] [CLS] 芳 幾 「 東海道 [MASK] 名所 [MASK] 内'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training models for masked language modeling, there are two techniques that can be used is to mask:\n",
        "\n",
        "1) mask individual tokens, like `##ontas` from word `kainrontas`\n",
        "\n",
        "2) mask whole words together (eg word `kainrontas`), not just individual tokens. This approach is called whole word masking. If we want to use whole word masking, we will need to build a data collator ourselves. \n",
        "\n",
        "We apply only the 1) as for now is complicate, because we do not have `tokinizer._isfast` to extracts `word_ids`"
      ],
      "metadata": {
        "id": "vwKyubIEy_i1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4) split dataset"
      ],
      "metadata": {
        "id": "AwVsgRux1I2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm_datasets[\"unsupervised\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQVC1Wgc3l7c",
        "outputId": "a141a756-fbf4-4364-a884-5ca575e8a202"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 2995\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f1fB0S4wYtQ",
        "outputId": "ae6f50f1-143b-48e8-e14b-f431fafa4c38"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    unsupervised: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 2995\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size = lm_datasets[\"unsupervised\"].num_rows\n",
        "train_size = int(0.8 * sample_size) # 10\n",
        "test_size = int(0.2 * sample_size)\n",
        "\n",
        "downsampled_dataset = lm_datasets[\"unsupervised\"].train_test_split(\n",
        "    train_size=train_size, test_size=test_size, seed=42\n",
        ")\n",
        "downsampled_dataset"
      ],
      "metadata": {
        "id": "Z8yf4KAN8AwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dafc7f43-5dc8-46b6-b923-2f401efae83d"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /root/.cache/huggingface/datasets/csv/default-f412c487edfea47d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b8392c1b4c36da23.arrow and /root/.cache/huggingface/datasets/csv/default-f412c487edfea47d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7daf0960e00092b1.arrow\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 2396\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 599\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Models"
      ],
      "metadata": {
        "id": "Gg8VtJga4I-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "model_name = 'bert-japanese'"
      ],
      "metadata": {
        "id": "KTXmZ_UIhUnD"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Including logging_steps to ensure we track the training loss with each epoch\n",
        "\n",
        "# Show the training loss with every epoch\n",
        "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
        "if logging_steps <= 0:\n",
        "    logging_steps=1"
      ],
      "metadata": {
        "id": "rRpmvKwE9pwc"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{model_name}-finetuned-meisho\",\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=True, # to enable mixed-precision training, which gives us another boost in speed \n",
        "    logging_steps=logging_steps, #to ensure we track the training loss with each epoch\n",
        "    #push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fJ2AJHb1DZ-",
        "outputId": "32b5dc1f-507b-4e6c-91b4-3c1088a4f5b8"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=downsampled_dataset[\"train\"],\n",
        "    eval_dataset=downsampled_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "1hYre4zr8AzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1952e3c8-8f17-4da5-c014-0fa418555d57"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# compute the resulting perplexity on the test set before fine-tune\n",
        "\n",
        "eval_results_before = trainer.evaluate() # compute the cross-entropy loss on the test set\n",
        "print(f\">>> Perplexity: {math.exp(eval_results_before['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "uSByRgsT8A26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "5652c646-122e-4a5d-d120-320761207792"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 599\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Perplexity: 4.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lower perplexity score means a better language model, so we check if with after fine-tuning has better results "
      ],
      "metadata": {
        "id": "XSCTHim-8-Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model (training mode)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "eut4PiMM8A65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "23cde80a-631a-4123-f6a0-af8b9a617b58"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 2396\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 114\n",
            "  Number of trainable parameters = 110650880\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [114/114 01:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.535400</td>\n",
              "      <td>1.312717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.360200</td>\n",
              "      <td>1.238028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.302700</td>\n",
              "      <td>1.226828</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 599\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:24]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 599\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 599\n",
            "  Batch size = 64\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=114, training_loss=1.3991141612069649, metrics={'train_runtime': 67.6707, 'train_samples_per_second': 106.22, 'train_steps_per_second': 1.685, 'total_flos': 472987207729152.0, 'train_loss': 1.3991141612069649, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the resulting perplexity on the test set\n",
        "eval_results = trainer.evaluate()"
      ],
      "metadata": {
        "id": "ISlgcZ5-8A96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "83a1a54a-034c-4389-feae-4ccf9c8c6b91"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 599\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\">>> Perplexity Before Fine-tune: {math.exp(eval_results_before['eval_loss']):.2f}\")\n",
        "print(f\">>> Perplexity After Fine-tune: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "vcm8d1lf8RBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1547222d-25a7-4128-d7f7-0a1feefbc3a2"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Perplexity Before Fine-tune: 4.66\n",
            ">>> Perplexity After Fine-tune: 3.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "QVakmal28RKq"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "66nhmCsg8RO1"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tranfer learning"
      ],
      "metadata": {
        "id": "82RbERuGTTmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Named Entity Recognition with Japanese BERT**\n"
      ],
      "metadata": {
        "id": "auyglIDGTipB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # create map between labels and an id\n",
        "# labels_to_ids = {k: v for v, k in enumerate(['O','PLACE'])}\n",
        "# ids_to_labels = {v: k for v, k in enumerate(['O','PLACE'])}"
      ],
      "metadata": {
        "id": "5UNzwwZLQV39"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels_to_ids"
      ],
      "metadata": {
        "id": "gqSMcFJ3TPrd"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DVCnhujU4TPR"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ziDjR5EM4TTR"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- load in the format that accept hugging face, using `load_dataset` (see tutorial: [Hugginface](https://huggingface.co/course/chapter5/2?fw=pt))\n",
        "\n",
        "more see fine tune of Japanese bert"
      ],
      "metadata": {
        "id": "dKl01bCz4YxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# url = \"\"\n",
        "# data_files = {\n",
        "#     \"train\": url + \"test_place.csv\",\n",
        "#     \"test\": url + \"train_place.csv\",\n",
        "# }\n",
        "\n",
        "# meisho_dataset = load_dataset(\"csv\", data_files=data_files)\n",
        "\n",
        "# meisho_dataset"
      ],
      "metadata": {
        "id": "uVsiteT-4TXZ"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# meisho_dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "WOPcL6Ec4Ta5"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample = meisho_dataset[\"train\"].shuffle(seed=SEED).select(range(3))\n",
        "\n",
        "# for row in sample:\n",
        "#     print(f\"\\n'>>> Title: {row['title']}'\")"
      ],
      "metadata": {
        "id": "4ogwu8U44TfB"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ideas"
      ],
      "metadata": {
        "id": "rVIMGS-qwzOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- use a model called [DistilBERT](https://huggingface.co/distilbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France.) that can be trained much faster with little to no loss in downstream performance. This model was trained using a special technique called [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation), where a large “teacher model” like BERT is used to guide the training of a “student model” that has far fewer parameters. An explanation of the details of knowledge distillation would take us too far afield in this section, but if you’re interested you can read all about it in *Natural Language Processing with Transformers* (colloquially known as the Transformers textbook)."
      ],
      "metadata": {
        "id": "PN2wccpkw5u_"
      }
    }
  ]
}